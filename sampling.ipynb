{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling to evaluate $P(\\mathbf{y}_t \\, | \\, \\mathbf{r}_t, S_t)$\n",
    "Recall that we assume that we are recording from $N$ neurons, and that we observe $K(t) = K$ marks in a time window $(t-1, t]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Goal:__ For the forward-backward algorithm, we need to be able to evaluate $b_j(\\mathbf{y}_t) \\equiv P(\\mathbf{y}_t \\, | \\, \\mathbf{r}_t , S_t=j) = P\\bigl( (m_k)_{k=1}^{K(t)} \\, \\big| \\, \\mathbf{r}_t^{(j)}; \\{\\boldsymbol{\\mu}_n, \\boldsymbol{\\Sigma}_n \\}_{n=1}^N \\bigr) = \\mathcal{L}\\bigl(\\mathbf{r}_t^{(j)} \\, \\big| \\, ( m_k )_{k=1}^{K(t)}; \\{\\boldsymbol{\\mu}_n, \\boldsymbol{\\Sigma}_n \\}_{n=1}^N \\bigr)$.\n",
    "\n",
    "Dropping the dependence of $K$ on $t$ (purely for notational simplicity), and using the shorthand $\\mathbf{r}_t^{(j)}$ to mean $\\mathbf{r}_t \\, | \\, S_t=j$, and dopping the explicit parametereization of $P(\\cdot)$ by $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$, we note that we desire to evaluate \n",
    "\\begin{equation}\n",
    "P_{\\mathbf{y}_t, | \\mathbf{r}_t^{(j)}} \\bigl( (m_k)_{k=1}^{K} \\, \\big| \\,\\mathbf{r}_t^{(j)} \\bigr).\n",
    "\\end{equation}\n",
    "\n",
    "We can introduce the auxiliary hidden random variable $\\mathbb{1}^K \\in \\mathbb{Z}^{N \\times K}$, where each column of $\\mathbb{1}^K$ is a standard unit vector $\\mathbf{e}_n$ whose elements are all zeros, except for the $n$th element, which is equal to one. That is, the $k$th column encodes the neuron identity $n$ that generated the $k$th mark. We denote this as $\\mathbb{1}^K \\equiv \\bigl(\\mathbf{e}_{u(k)} \\bigr)_{k=1}^K$, such that $u(k)\\in \\mathbb{Z}$ is the neuron identity of the $k$th mark.\n",
    "\n",
    "Now clearly,\n",
    "\\begin{equation}\n",
    "P_{\\mathbf{y}_t, | \\mathbf{r}_t^{(j)}} \\bigl( (m_k)_{k=1}^{K} \\, \\big| \\,\\mathbf{r}_t^{(j)} \\bigr) = \\int_{\\mathbb{1}^K}\\, P_{\\mathbf{y}_t, \\mathbb{1}^K | \\mathbf{r}_t^{(j)}} \\bigl( (m_k)_{k=1}^{K}, \\mathbb{1}^K \\, \\big| \\,\\mathbf{r}_t^{(j)} \\bigr) \\, \\text{d}\\mathbb{1}^K\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that\n",
    "\\begin{equation}\n",
    "P_{\\mathbf{y}_t, \\mathbb{1}^K | S_t, \\mathbf{r}_t} \\bigl( (m_k)_{k=1}^{K}, \\mathbb{1}^K  \\, \\big| \\, S_t, \\mathbf{r}_t \\bigr) = P_{\\mathbf{y}_t | \\mathbb{1}^K, S_t, \\mathbf{r}_t} \\bigl( (m_k)_{k=1}^{K}  \\, \\big| \\, \\mathbb{1}^K, S_t, \\mathbf{r}_t \\bigr) \\cdot P_{\\mathbb{1}^K|S_t, \\mathbf{r}_t}(\\mathbb{1}^K \\, | \\, S_t, \\mathbf{r}_t)\n",
    "\\end{equation}\n",
    "so that\n",
    "\\begin{equation}\n",
    "\\int_{\\mathbb{1}^K} \\, P_{\\mathbf{y}_t, \\mathbb{1}^K | S_t, \\mathbf{r}_t} \\bigl( (m_k)_{k=1}^{K}, \\mathbb{1}^K  \\, \\big| \\, S_t, \\mathbf{r}_t \\bigr) \\, \\text{d}\\mathbb{1}^K = \\int_{\\mathbb{1}^K} \\, P_{\\mathbf{y}_t | \\mathbb{1}^K, S_t, \\mathbf{r}_t} \\bigl( (m_k)_{k=1}^{K}  \\, \\big| \\, \\mathbb{1}^K, S_t, \\mathbf{r}_t \\bigr) \\cdot P_{\\mathbb{1}^K|S_t, \\mathbf{r}_t}(\\mathbb{1}^K \\, | \\, S_t, \\mathbf{r}_t) \\, \\text{d}\\mathbb{1}^K\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "\\text{LHS} = \\int_{\\mathbb{1}^K} \\, P_{\\mathbf{y}_t | \\mathbb{1}^K, S_t, \\mathbf{r}_t} \\bigl( (m_k)_{k=1}^{K}  \\, \\big| \\, \\mathbb{1}^K, S_t, \\mathbf{r}_t \\bigr) \\cdot P_{\\mathbb{1}^K|S_t, \\mathbf{r}_t}(\\mathbb{1}^K \\, | \\, S_t, \\mathbf{r}_t) \\, \\text{d}\\mathbb{1}^K = \\mathbb{E}_{\\mathbb{1}^K|S_t, \\mathbf{r}_t} \\bigl[ P_{\\mathbf{y}_t | \\mathbb{1}^K, S_t, \\mathbf{r}} \\bigl( (m_k)_{k=1}^K \\, \\big| \\, \\mathbb{1}^K, S_t, \\mathbf{r}_t  \\bigr) \\bigr] = \\text{RHS}\n",
    "\\end{equation}\n",
    "Further, note that\n",
    "\\begin{equation}\n",
    "P_{\\mathbf{y}_t | \\mathbb{1}^K, S_t, \\mathbf{r}_t} \\bigl( (m_k)_{k=1}^{K}  \\, \\big| \\, \\mathbb{1}^K, S_t, \\mathbf{r}_t \\bigr) = P_{\\mathbf{y}_t | \\mathbb{1}^K} \\bigl( (m_k)_{k=1}^{K}  \\, \\big| \\, \\mathbb{1}^K\\bigr) = \\prod_{k=1}^K \\mathcal{N} \\bigl(m_k; \\boldsymbol{\\mu}_{u(k)}, \\boldsymbol{\\Sigma}_{u(k)}\\bigr)\n",
    "\\end{equation}\n",
    "and that\n",
    "\\begin{equation}\n",
    "P_{\\mathbb{1}^K|S_t, \\mathbf{r}_t}(\\mathbb{1}^K \\, | \\, S_t, \\mathbf{r}_t) = \\prod_{n=1}^N \\text{Pois}\\bigl(V_n; r_t^n\\bigr) \\quad \\bigl[\\text{that is}, V_n \\, | \\, r_t^n \\sim \\text{Pois}(r_t^n) \\quad \\text{s.t.}~\\sum_{n=1}^N V_n = K\\bigr]\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "V_n = \\sum_{k=1}^K\\bigl(u(k)=n\\bigr), \\quad n=1,2,\\ldots, N\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is generally difficult to compute the integral (LHS) directly, whereas it is simpler to estimate the expected value (RHS), assuming of course, that we can sample from $\\mathbb{1}^K\\, |\\, S_t, \\mathbf{r}_t$ according to it's distribution.\n",
    "\n",
    "Indeed, if we are able to sample $\\mathbf{W}_i \\sim P_{\\mathbb{1}^K|S_t, \\mathbf{r}_t}(\\mathbb{1}^K \\, | \\, S_t, \\mathbf{r}_t)$, where $\\mathbf{W}_i \\in \\mathbb{Z}^{N \\times K}$ for $i=1,\\ldots, M$, then\n",
    "\\begin{align}\n",
    "P_{\\mathbf{y}_t, | S_t, \\mathbf{r}_t} \\bigl( (m_k)_{k=1}^{K} \\, \\big| \\, S_t, \\mathbf{r}_t \\bigr) &= \\mathbb{E}_{\\mathbb{1}^K|S_t, \\mathbf{r}_t} \\bigl[ P_{\\mathbf{y}_t | \\mathbb{1}^K, S_t, \\mathbf{r}} \\bigl( (m_k)_{k=1}^K \\, \\big| \\, \\mathbb{1}^K, S_t, \\mathbf{r}_t  \\bigr) \\bigr]\\\\ &\\approx \\dfrac{1}{M} \\sum_{i=1}^M P_{\\mathbf{y}_t | \\mathbb{1}^K, S_t, \\mathbf{r}} \\bigl( (m_k)_{k=1}^K \\, \\big| \\, \\mathbb{1}^K=\\mathbf{W}_i, S_t, \\mathbf{r}_t  \\bigr) \\\\\n",
    "&= \\dfrac{1}{M}\\sum_{i=1}^M P_{\\mathbf{y}_t | \\mathbf{W}} \\bigl( (m_k)_{k=1}^K \\, \\big| \\, \\mathbf{W}_i\\bigr) \\nonumber \\\\\n",
    "&= \\dfrac{1}{M} \\sum_{i=1}^M\\prod_{k=1}^K \\mathcal{N}\\bigl(m_k; \\boldsymbol{\\mu}(\\mathbf{w}_k^{(i)}), \\boldsymbol{\\Sigma}(\\mathbf{w}_k^{(i)}) \\bigr)\n",
    "\\end{align}\n",
    "where, for each mark, $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$ depend on the neuron identity encoded by $\\mathbf{w}_k^{(i)}$, the $k$th column of the $i$th sample $\\mathbf{W}_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __So how can we sample $\\mathbf{W}_i$ according to it's underlying distribution?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONTINUE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making sure things are correct\n",
    "\n",
    "Recall the conditional expectation $\\mathbb{E}_Y [ Y\\, |\\, X=x ] = \\int_{-\\infty}^\\infty\\, y \\cdot f(y\\, |\\, x) \\, \\text{d}y$. It seems nonsensical to consider $\\mathbb{E}_Y[Y=y\\,|\\, X]$, which simply equals $y$, because $Y$ has been fixed, independent of $X$. But what about $\\mathbb{E}_X [ Y=y \\, | \\, X]$? This latter quantity also seems either nonsensical, or pathological. Luckily we don't have to worry about them too much, because we are interested in computing expectaitons of the form $\\mathbb{E}_X[P(Y=y\\, | \\, X)]$, which is only superficially similar to $\\mathbb{E}_X[Y=y \\, | \\, X]$. Nevertheless, I wanted to make doubly sure that $\\mathbb{E}_X[P(Y=y\\, | \\, X)]$ is a reasonable thing to consider, and that I knew how to compute it$\\ldots$\n",
    "\n",
    "We want to compute things of the form $\\mathbb{E}_X \\bigl[ P(Y=y\\, | \\, X) \\bigr]$. Note that $P(Y=y\\, | \\, X)$ is an $X$-measurable function, with $Y$ fixed to the value $y$. That is, $P(Y=y\\, | \\, X)$ is a random variable with sigma field $\\sigma({X})$.\n",
    "\n",
    "In this case, we have  \n",
    "\n",
    "__Option A__\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_X \\bigl[ P(Y=y\\, | \\, X) \\bigr] = \\int_X P_{Y|X}(Y=y|X=x)\\cdot P_X(x) \\, \\text{d}x\n",
    "\\end{equation}\n",
    "\n",
    "__Option B__\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_X \\bigl[ P(Y=y\\, | \\, X) \\bigr] = \\int_X P_{Y|X}(Y=y|X=x)\\cdot P_{X|Y}(x\\, |\\, y) \\, \\text{d}x\n",
    "\\end{equation}\n",
    "\n",
    "I am confident that __Opetion A__ is correct. Indeed, $\\int_Y \\mathbb{E}_X \\bigl[P(y \\, | \\, X)\\bigr] \\, \\text{d}y = 1$, which is easy to prove using the linearity of Expectation, so that this implies that __Option A__ is indeed correct (not proven here).\n",
    "\n",
    "Then our sampling strategy to approximate the expectation becomes\n",
    "\\begin{align}\n",
    "x_i &\\sim P_X, \\quad i=1,\\ldots, M \\\\\n",
    "\\mathbb{E}_X\\bigl[ P(Y=y \\, | \\, X) \\bigr] &\\approx \\dfrac{1}{M} \\sum_{i=1}^M P(Y=y \\, | \\, x_i)\n",
    "\\end{align}\n",
    "\n",
    "### So what?\n",
    "The interesting implication from the above is that when we evaluate $P\\bigl((m_k)_{k=1}^K \\, \\big| \\, S_t, \\mathbf{r}_t \\bigr)$ by approximating $\\mathbb{E}_{\\mathbb{1}^K|S_t, \\mathbf{r}_t} \\bigl[ P_{\\mathbf{y}_t | \\mathbb{1}^K, S_t, \\mathbf{r}} \\bigl( (m_k)_{k=1}^K \\, \\big| \\, \\mathbb{1}^K, S_t, \\mathbf{r}_t  \\bigr) \\bigr]$, we need to sample $\\mathbb{1}^K | \\mathbf{r}_t$, _independent_ from the observed marks (but amusingly, dependent on the _number_ of observed marks)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD CONTENT BELOW; DO NOT TRUST!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall\n",
    "\\begin{equation}\n",
    "f(y,x) = f(x) \\cdot f(y|x).\n",
    "\\end{equation}\n",
    "\n",
    "The marginal density of $Y$ can then be obtained from\n",
    "\\begin{equation}\n",
    "f(y) = \\int_{-\\infty}^\\infty\\, f(x) \\cdot f(y|x) \\, \\text{d}x.\n",
    "\\end{equation}\n",
    "\n",
    "The conditional expectation is\n",
    "\\begin{equation}\n",
    "\\mathbb{E} [ Y|X=x ] = \\int_{-\\infty}^\\infty\\, y \\cdot f(y|x) \\, \\text{d}y.\n",
    "\\end{equation}\n",
    "\n",
    "and the unconditional expectation of $Y$ is\n",
    "\\begin{equation}\n",
    "\\mathbb{E}[Y] = \\int_{-\\infty}^\\infty\\, \\mathbb{E}[Y|X=x] \\cdot f(x) \\, \\text{d}x.\n",
    "\\end{equation}\n",
    "\n",
    "---\n",
    "\n",
    "Let $X$, $Y$ be dependent continuous RVs.\n",
    "\n",
    "Then $P(Y|X=x)$ is a RV as well as a function of $y$ for some fixed $X=x$.\n",
    "\n",
    "Let $h(y;x) \\equiv P(Y=y|X=x)$. Then\n",
    "\\begin{align}\n",
    "\\mathbb{E}_Y[h(y;x)] &= \\int_Y h(y;x) \\cdot f(y;x) \\, \\text{d} y \\\\\n",
    "&= \\int_Y P(Y=y|X=x) \\cdot P(Y=y|X=x) \\, \\text{d} y\n",
    "\\end{align}\n",
    "so that\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_Y[P(Y|X=x)] = \\int_Y P(y|X=x)^2 \\, \\text{d}y\n",
    "\\end{equation}\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Goal:__ Recall that for the forward-backward algorithm, we need to be able to evaluate $b_j(\\mathbf{y}_t) \\equiv P(\\mathbf{y}_t \\, | \\, \\mathbf{r}_t , S_t=j) = P\\bigl( (m_k)_{k=1}^{K(t)} \\, \\big| \\, \\mathbf{r}_t^{(j)}; \\{\\boldsymbol{\\mu}_n, \\boldsymbol{\\Sigma}_n \\}_{n=1}^N \\bigr) = \\mathcal{L}\\bigl(\\mathbf{r}_t^{(j)} \\, \\big| \\, ( m_k )_{k=1}^{K(t)}; \\{\\boldsymbol{\\mu}_n, \\boldsymbol{\\Sigma}_n \\}_{n=1}^N \\bigr)$.\n",
    "\n",
    "We do not have an easy way to directly evaluate this density function (likelihood).\n",
    "\n",
    "But notice that if we knew which neurons fired during the observation window $(t-1, t]$, then we should be able to evaluate  COMPLETE ME!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "I need to be even more clear / explicit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\int_{\\mathbb{1}^K} \\, P_{\\mathbf{y}_t | \\mathbb{1}^K, S_t, \\mathbf{r}_t} \\bigl( (m_k)_{k=1}^{K}  \\, \\big| \\, \\mathbb{1}^K, S_t, \\mathbf{r}_t \\bigr) \\cdot P_{\\mathbb{1}^K|S_t, \\mathbf{r}_t}(\\mathbb{1}^K \\, | \\, S_t, \\mathbf{r}_t) \\, \\text{d}\\mathbb{1}^K &= \\mathbb{E}_{\\mathbb{1}^K|S_t, \\mathbf{r}_t} \\bigl[ P_{\\mathbf{y}_t | \\mathbb{1}^K, S_t, \\mathbf{r}} \\bigl( (m_k)_{k=1}^K \\, \\big| \\, \\mathbb{1}^K, S_t, \\mathbf{r}_t  \\bigr) \\bigr] \\\\\n",
    "&= P_{\\mathbf{y}_t|S_t, \\mathbf{r}_t} \\bigl( (m_k)_{k=1}^K \\, \\big| \\, S_t, \\mathbf{r}_t \\bigr) \\\\\n",
    "&= b_j(\\mathbf{y}_t)\n",
    "\\end{align}\n",
    "\n",
    "So we need to evaluate $P_{\\mathbf{y}_t | \\mathbb{1}^K, S_t, \\mathbf{r}} \\bigl( (m_k)_{k=1}^K \\, \\big| \\, \\mathbb{1}^K, S_t, \\mathbf{r}_t  \\bigr)$ by sampling from $\\color{red}{\\mathbb{1}^K| (m_k)_{k=1}^K, S_t, \\mathbf{r}_t}$, for which\n",
    "\\begin{align}\n",
    "P_{\\mathbf{y}_t | \\mathbb{1}^K, S_t, \\mathbf{r}} \\bigl( (m_k)_{k=1}^K \\, \\big| \\, \\mathbb{1}^K, S_t, \\mathbf{r}_t  \\bigr) &= P_{\\mathbf{y}_t | \\mathbb{1}^K} \\bigl( (m_k)_{k=1}^K \\, \\big| \\, \\mathbb{1}^K \\bigr)\\\\\n",
    "&= \\prod_{k=1}^K \\mathcal{N} \\bigl(m_k; \\boldsymbol{\\mu}_{u(k)}, \\boldsymbol{\\Sigma}_{u(k)}\\bigr)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Sampling conditional neuron IDs__\n",
    "\n",
    "So our goal becomes: sample $\\mathbb{1}^K \\, \\big| \\, (m_k)_{k=1}^K, \\mathbf{r}_t, S_t=j; \\{\\boldsymbol{\\mu}_n, \\boldsymbol{\\Sigma}_n \\}_{n=1}^N$, where $\\mathbb{1}^K \\equiv \\bigl(\\mathbf{e}_{u(k)} \\bigr)_{k=1}^K$. That is, we want to sample a sequence of unit vectors which contain the neuron identities for the associated marks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each window $(t-1, t], \\quad t=1,\\ldots, T$:  \n",
    "For each state $S_t = j, \\quad j=1,\\ldots Z$:  \n",
    "For each mark $m_k, \\quad k=1,\\ldots K(t)$:  \n",
    "sample $\\mathbf{e}_{u(k)}$:  \n",
    "\\begin{equation}\n",
    "u(k) \\sim \\text{Multinomial}(\\mathbf{p})\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "p_i = \\dfrac{\\mathcal{N}\\bigl(m_k; \\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i \\bigr)\\cdot r_t^i}{\\sum_{n=1}^N \\mathcal{N}\\bigl(m_k; \\boldsymbol{\\mu}_n, \\boldsymbol{\\Sigma}_n \\bigr) \\cdot r_t^n}, \\quad i=1,2,\\ldots, N\n",
    "\\end{equation}\n",
    "Then, with $\\bigl(\\mathbf{e}_{u(k)} \\bigr)_{k=1}^K$, we can evaluate\n",
    "\\begin{align}\n",
    "P_{\\mathbf{y}_t | \\mathbb{1}^K, S_t, \\mathbf{r}} \\bigl( (m_k)_{k=1}^K \\, \\big| \\, \\mathbb{1}^K, S_t, \\mathbf{r}_t  \\bigr) &= P_{\\mathbf{y}_t | \\mathbb{1}^K} \\bigl( (m_k)_{k=1}^K \\, \\big| \\, \\mathbb{1}^K \\bigr)\\\\\n",
    "&= P_{\\mathbf{y}_t | \\mathbb{1}^K} \\bigl( (m_k)_{k=1}^K \\, \\big| \\, \\bigl(\\mathbf{e}_{u(k)} \\bigr)_{k=1}^K \\bigr)\\\\\n",
    "&= \\prod_{k=1}^K \\mathcal{N} \\bigl(m_k; \\boldsymbol{\\mu}_{u(k)}, \\boldsymbol{\\Sigma}_{u(k)}\\bigr)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but each sequence of samples $\\bigl(\\mathbf{e}_{u(k)} \\bigr)_{k=1}^K$ is also a sample from the above distribution (as a function of $\\mathbb{1}^K$), so that summing the $M$ samples gives us\n",
    "\\begin{equation}\n",
    "x_i \\sim P_{\\mathbf{y}_t | \\mathbb{1}^K, S_t, \\mathbf{r}_t} \\bigl( (m_k)_{k=1}^{K}  \\, \\big| \\, \\mathbb{1}^K, S_t, \\mathbf{r}_t \\bigr) = f\\bigl(\\mathbb{I}^K\\bigr) \\implies \\dfrac{1}{M} \\sum_{i=1}^M x_i =\\mathbb{E}_{\\mathbb{1}^K|S_t, \\mathbf{r}_t} \\bigl[ P_{\\mathbf{y}_t | \\mathbb{1}^K, S_t, \\mathbf{r}} \\bigl( (m_k)_{k=1}^K \\, \\big| \\, \\mathbb{1}^K, S_t, \\mathbf{r}_t  \\bigr) \\bigr] = P(\\mathbf{y}_t \\, | \\, \\mathbf{r}_t , S_t=j)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that\n",
    "\\begin{equation}\n",
    "P_{\\mathbf{y}_t | \\mathbb{1}^K, S_t, \\mathbf{r}_t} \\bigl( (m_k)_{k=1}^{K}  \\, \\big| \\, \\mathbb{1}^{\\tilde{K}}, S_t, \\mathbf{r}_t \\bigr) = 0, \\quad \\text{whenever } K \\neq \\tilde{K}\n",
    "\\end{equation}\n",
    "That is, when we sample $\\mathbb{1}^K$ to approximate the above equations, we are constrained by the total number of observed spikes, $K$, and not merely by the firing rates. This effectively restricts the space over which we need to integrate, or sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we simply wanted to sample $V| S_t, \\mathbf{r}_t$, we could simply sample from the $N$ Poisson distributions independently. But we actually want to impose the constraint that $\\sum_{n=1}^N V_n = K$, and we also want to generate $\\mathbb{1}^K$, which is ordered, in contrast to $V$. By adding in the summation constraint, we can instead sample $V| S_t, \\mathbf{r}_t$ be repeatedly drawing ($K$ times) from a multinomial distribution, with component probablities proportional to the Poisson rates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
